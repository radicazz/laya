name: Benchmark

on:
  workflow_run:
    workflows: ["Build"]
    types:
      - completed

env:
  ACT: 'false'

jobs:
  benchmark-linux:
    name: Benchmark-${{ matrix.compiler }}-Release
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'success'

    strategy:
      fail-fast: false
      matrix:
        compiler: [gcc, clang]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download build artifacts
      if: ${{ env.ACT != 'true' }}
      uses: actions/download-artifact@v4
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        name: laya-linux-${{ matrix.compiler }}-Release
        run-id: ${{ github.event.workflow_run.id }}
        path: build

    - name: Install Ubuntu dependencies
      run: ./scripts/ci/install-ubuntu-deps.sh --runtime

    - name: Setup headless display
      run: ./scripts/ci/setup-xvfb.sh

    - name: Make executables runnable
      run: |
        chmod +x build/tests/laya_tests_benchmark

    - name: Run benchmarks
      run: |
        eval "$(./scripts/ci/find-sdl3-lib.sh)"
        ./scripts/ci/run-benchmarks-linux.sh \
          --compiler ${{ matrix.compiler }} \
          --build-type Release \
          --no-sdl-search

    - name: Upload benchmark results
      if: ${{ env.ACT != 'true' }}
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-linux-${{ matrix.compiler }}-release
        path: benchmark-results/
        retention-days: 365  # Extended retention for historical comparison

    - name: Cleanup headless display
      if: always()
      run: ./scripts/ci/cleanup-xvfb.sh

  # Aggregate all benchmark results into a single artifact
  aggregate-results:
    name: Aggregate Benchmark Results
    runs-on: ubuntu-latest
    needs: [benchmark-linux]
    if: github.event.workflow_run.conclusion == 'success'

    steps:
    - name: Download all benchmark results
      if: ${{ env.ACT != 'true' }}
      uses: actions/download-artifact@v4
      with:
        pattern: benchmark-results-*
        path: all-benchmarks/

    - name: Create summary
      run: |
        mkdir -p aggregated-benchmarks

        # Move all results into aggregated folder
        find all-benchmarks -type f -exec cp {} aggregated-benchmarks/ ';'

        # Create summary README
        cat > aggregated-benchmarks/README.md << EOF
        # Benchmark Results

        **Commit**: ${{ github.sha }}
        **Branch**: ${{ github.ref_name }}
        **Timestamp**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
        **Workflow Run**: ${{ github.run_number }}

        ## Available Results

        - Linux GCC Release: \`benchmarks-gcc-release.json\`, \`benchmarks-gcc-release.txt\`
        - Linux Clang Release: \`benchmarks-clang-release.json\`, \`benchmarks-clang-release.txt\`

        ## Metadata Files

        - \`metadata-gcc.txt\`
        - \`metadata-clang.txt\`

        ## Usage

        JSON files can be parsed programmatically for historical comparison and visualization.
        Text files provide human-readable output for quick review.
        EOF

        # List all files
        ls -lah aggregated-benchmarks/

    - name: Upload aggregated results
      if: ${{ env.ACT != 'true' }}
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-all-compilers
        path: aggregated-benchmarks/
        retention-days: 365  # Extended retention for historical comparison
